{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb的优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、正则化\n",
    "标准的GBM算法里没有正则化，因此xgb能够更好的减少过拟合的风险，而且xgb也被认为是 'regularized boosting'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、并行计算 虽然boosting算法是需要一步接着一步的，xgb可以在建立一个树的时候并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、灵活 允许自己添加优化目标和评价指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4、剪枝 xgb与gbm停止的策略不一样。gbm是贪心策略，如果某一次分裂并没有使结果更好，gbm就会在此时停止分裂。xgb会一直分裂到预设的max_depth，然后开始回头剪纸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、内置交叉验证 在每一轮迭代的时候，都可以使用交叉验证，这样在结束的时候就能看出来到底迭代几轮模型有更好的泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、在线学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb参数被分成了三大类：\n",
    "    1、General Parameters: Guide the overall functioning\n",
    "    2、Booster Parameters: Guide the individual booster (tree/regression) at each step\n",
    "    3、Learning Task Parameters: Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "booster [default=gbtree]\n",
    "    Select the type of model to run at each iteration. It has 2 options:\n",
    "        gbtree: tree-based models\n",
    "        gblinear: linear models\n",
    "每一步的弱学习器的种类，默认是树模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "silent [default=0]:\n",
    "    Silent mode is activated is set to 1, i.e. no running messages will be printed.\n",
    "    It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "迭代的时候是否在每一轮输出中间过程"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nthread [default to maximum number of threads available if not set]\n",
    "    This is used for parallel processing and number of cores in the system should be entered\n",
    "    If you wish to run on all cores, value should not be entered and algorithm will detect automatically\n",
    "进程的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booster Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然有两种可以选择的弱学习器(booster)，但是一般使用树模型(tree booster)，所以下面的参数都是针对树模型的。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eta [default=0.3]\n",
    "    Analogous to learning rate in GBM\n",
    "    Makes the model more robust by shrinking the weights on each step\n",
    "    Typical final values to be used: 0.01-0.2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率，每一次迭代的时候都会收缩权重，使得模型的鲁棒性更强。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "min_child_weight [default=1]\n",
    "    Defines the minimum sum of weights of all observations required in a child.\n",
    "    This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations   while GBM has min “number of observations”.\n",
    "    Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    Too high values can lead to under-fitting hence, it should be tuned using CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.在分裂的时候，如果一个节点所有的样本的权重之和小于min_child_weight，那么就停止分裂\n",
    "2.增大这个数值，防止过拟合"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_depth [default=6]\n",
    "    The maximum depth of a tree, same as GBM.\n",
    "    Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    Should be tuned using CV.\n",
    "    Typical values: 3-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.树的深度\n",
    "2.减小，防止过拟合"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_leaf_nodes\n",
    "    The maximum number of terminal nodes or leaves in a tree.\n",
    "    Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    If this is defined, GBM will ignore max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.每棵树拥有的最大的叶子节点的数目2.减小防止过拟合3.二叉树的叶子节点数目和深度可以互相求出来，所以这个参数和上面的max_depth只需定义一个"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gamma [default=0]\n",
    "    A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.每次分裂节点都需要使得模型的表现有所提升，gamma用来规定提升的数值，只有大于gamma的分裂，才会让这个节点分裂下去 2.增大减少过拟合"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "max_delta_step [default=0]\n",
    "    In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    This is generally not used but you can explore further if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、权重改变最大步长？？2、一般用不到，只有在逻辑斯蒂回归解决极端类别不平衡问题的时候"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subsample [default=1]\n",
    "    Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    Typical values: 0.5-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、类似于bagging算法里的随机采样2、更小的数值减少过拟合"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colsample_bytree [default=1]\n",
    "    Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    Typical values: 0.5-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、同样类似于bagging算法，每次训练用的特征的比例 2、减小，防止过拟合 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "colsample_bylevel [default=1]\n",
    "    Denotes the subsample ratio of columns for each split, in each level.\n",
    "    I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、每层随机采样列数占比，跟上面那个有区别？？？"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lambda [default=1]\n",
    "    L2 regularization term on weights (analogous to Ridge regression)\n",
    "    This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2正则化前面的系数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "alpha [default=0]\n",
    "    L1 regularization term on weight (analogous to Lasso regression)\n",
    "    Can be used in case of very high dimensionality so that the algorithm runs faster when implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1正则化的系数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scale_pos_weight [default=1]\n",
    "    A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理类别不平衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Learning Task Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "objective [default=reg:linear]\n",
    "    This defines the loss function to be minimized. Mostly used values are:\n",
    "    binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "    multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "    you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "    multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要被优化的目标函数"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eval_metric [ default according to objective ]\n",
    "    The metric to be used for validation data.\n",
    "    The default values are rmse for regression and error for classification.\n",
    "    Typical values are:\n",
    "    rmse – root mean square error\n",
    "    mae – mean absolute error\n",
    "    logloss – negative log-likelihood\n",
    "    error – Binary classification error rate (0.5 threshold)\n",
    "    merror – Multiclass classification error rate\n",
    "    mlogloss – Multiclass logloss\n",
    "    auc: Area under the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证的时候，采用什么样的度量标准。对回归问题默认是rmse，分类使用准确率。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "seed [default=0]\n",
    "    The random number seed.\n",
    "    Can be used for generating reproducible results and also for parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机数种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些参数的人生经验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth:3-10.开始的时候可以选在4-6之间\n",
    "min_child_weight:1如果存在数据不平衡\n",
    "gamma:0.1-0.2\n",
    "subsample,colsample_bytree = 0.8 0.5-0.9\n",
    "scale_pos_weight:1 数据不平衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参的顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.先确定学习率和迭代的轮数，即先确定集成学习框架类的参数。一般而言学习率在0.05到03之间，更确切的说是在0.1左右。根据学习率，采用xgb里的cv方法去决定迭代的轮数\n",
    "2.调整弱学习器的参数\n",
    "3.调整正则化的参数\n",
    "4.降低学习率，进一步调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
